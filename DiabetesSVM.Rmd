---
title: "Predire le Diabete"
author: "Joao BABADOUDOU"
date: "`r Sys.Date()`"
output: pdf_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Le diabète est un problème de sante publique majeur dans le monde entier, touchant des millions de personnes et ayant un impact significatif sur leur qualité de vie. Dans cette étude nous proposons une approche basée sur les machines a vecteurs de support (SVM) pour détecter la maladie. Pour cela nous allons utiliser des données récupérées sur Kaggle, présentant certaines caractéristiques tels que la pression sanguine ou la quantité d'insuline dans le sang de certaines personnes.

Commençons par charger toutes les bibliothèques dont on aura besoin et les données. Les données sont disponibles sur mon github.

```{r chunk 1, echo=TRUE, message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(GGally)
library(e1071)
library(caret)
library(gt) 
library(skimr)
# Importons nos données
df <- read_delim("diabetes.csv", delim = ";", 
                       escape_double = FALSE, col_types = cols(Outcome = col_factor(levels = c("0", 
                                                                                               "1"))), trim_ws = TRUE)
```

## **1- Analyse exploratoire des données**

### **1-1) Présentation du jeu de données**

### **1-2) Analyse univariable**

#### **1-2-1) Statistiques descriptives**

```{r}
 my_skim <- skim_with(numeric = sfl( p50 = NULL,hist=NULL, n_missing=NULL, complete_rate=NULL)) 
 diabetes_df <- my_skim(df)
 
 diabetes_df %>%
   select(-skim_type)   %>% 
   gt() %>%
   cols_label(
              numeric.mean = "Moyenne", numeric.sd = "Ecart-type",
              numeric.p0 = "Min",numeric.p25="1er Quartile" ,numeric.p75="3ème Quartile" ,
              numeric.p100 = "Max") %>%
   opt_stylize(style = 6, color = "cyan", add_row_striping = TRUE) %>%
   tab_header(title = "Summary of Variables in the diqbeyes data") 
```

L'analyse du tableau nous permet de constater l'absence de valeurs manquantes et des ecarts-types généralement élevés, surtout pour la quantité d'Insuline dans le sang. \#### **1-2-2) Distribution des variables**

Affichons maintenant la distribution de chacune des variables exceptés la variable explicative.

```{r}
c2    <-    ggplot(df,    aes(Pregnancies))    +
  geom_bar(fill    =    "#104E8B")   +
  ggtitle(names(df[1]))

d2    <-    ggplot(df,    aes(Glucose))    +
  geom_histogram( fill    =    "#104E8B")   +
  ggtitle(names(df[,2]))

e2    <-    ggplot(df,    aes(BloodPressure) )   +
  geom_histogram( fill    =    "#104E8B" )      +    ggtitle(names(df[,3]))

f2    <-    ggplot(df,    aes(SkinThickness))    +
  geom_histogram( fill    =    "#104E8B")    + 
  ggtitle(names(df[,4]))

g2    <-    ggplot(df,    aes(Insulin))    +
  geom_histogram( fill    =    "#104E8B")    +    ggtitle(names(df[,5]))

h2    <-    ggplot(df,    aes(BMI))    +
  geom_histogram(fill    =    "#104E8B")     +
  ggtitle(names(df[,6]))

i2    <-    ggplot(df,    aes(DiabetesPedigreeFunction))    +
  geom_histogram(fill    =    "#104E8B")    +   ggtitle(names(df[,7]))

j2    <-    ggplot(df,    aes(Age))    +
  geom_histogram(fill    =    "#104E8B")    + 
  ggtitle(names(df[,8]))


gridExtra::grid.arrange(c2,d2,e2,f2,g2,h2,i2,j2)


```

On peut constater que les variables ont pour la plupart une distribution asymétrique à gauche. C'est à dire que la majorité des valeurs de la variable est supérieure à la valeur moyenne. Il conviendra pour nous de procéder a une standardisation des données pour la suite des analyses

#### **1-2-3) Variable explicatives**

### **1-3) Analyse multivariee**

Sortons dans un premier temps une matrice de corrélation pour vérifier s'il y a des relations linéaires significatives entre les variables explicatives.

```{r}
ggcorr(df[,-9],low = "#104E8B",mid= "#FDF5E6", high="darkgreen", label = TRUE,label_size = 3)

```

Constatons que les variables **Age** et **Pregnancies** ont une relation linéaire plus ou moins forte. Concernant les autres variables la linéarité de leurs relation est négligeable au vu de la valeur des coefficients.


Avec le fonction **ggpairs** de [@GGally] nous allons largement résumer notre analyse multivariée et tirer les informations nécessaires.

```{r message=FALSE, warning=FALSE}
ggpairs(df, aes(colour = Outcome, alpha = 0.4))
```


 Quand on analyse la densité de chacune des variables repartit suivant **Outcome** , on constate un déphasage entre les courbes de **1** et celles de **0**. En effet ses figures, posées en diagonale, montrent que toutes les variables choisies sont importante dans la détection du diabète. On peut néanmoins remarquer que la quantité de glucose dans le sang et l'age sont des facteurs déterminant du diabète.
## **2- Modelisation**

### **2-1) Preprocessing**

Ici, nous allons préparer nos données a être utiliser dans le modèle. La première étape sera la normalisation des données. 

**Normalisation des donnees**

Une étape	très importante consiste	à	normaliser d’abord les valeurs des variables quantitatives. Toutes les mesures seront alors placées sur un	pied	d’égalité. Du	 coup, si certaines d’entre elles sont très petites, elles ne seront pas « oubliées »	parmi d'autres mesures bien plus grandes.
Nous allons utiliser la normalisation min-max. Ce processus transforme les variables de sorte que toutes les valeurs se situent dans la plage comprise entre 0 et 1.


```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x))) 
}

df[,1:8] <- apply(df[,1:8],MARGIN = 2 , FUN = normalize)

```


**Séparation des donnes en données d'apprentissage et donnes de test**

Nous allons séparer nos donnees en deux partie. Réservons 75% pour l’entraînement et donc 25% pour le test du modèle

```{r}

split_size	=	0.75
sample_size	=	floor(split_size	*	nrow(df)) 
set.seed(123)
train_indices	<-	sample(seq_len(nrow(df)),	size	=	
                          sample_size)
train	<-	df[train_indices,	] 
test	<-	df[-train_indices,	]
```


### **2-1) Entrainement du modele**

 Nous allons faire recourt à la fonction **train** du package [@caret] pour trouver notre modèle. A l'aide de la recherche aléatoire et d'une cross validation nous allons déterminer les paramètres de notre modèle. En effet, la recherche aléatoire est souvent efficace, car elle explore l'espace des hyperparamètres de manière aléatoire plutôt que de manière systématique comme la recherche simple. Cela permet de couvrir une plus grande variété de combinaisons d'hyperparamètres en moins d'itérations.


```{r}
grid <- expand.grid(C = runif(20, 0.1, 10), kernel = c('linear', 'radial'))


Tuned_model <- train(Outcome ~ ., data = train, method = 'svmRadial', tuneGrid = grid, trControl = 
trainControl(method = 'cv', number = 5))




```


```{r}


tuned_model <- tune(svm, Outcome ~ ., data = train, kernel ='linear', ranges = list(C = runif(20, 0.1, 10)), 
tunecontrol = tune.control(sampling = "cross", cross = 5))

best_C <- tuned_model$best.parameters$C
best_kernel <- tuned_model$best.parameters$kernel
# Afficher les meilleurs hyperparamètres trouvés 
print(tuned_model$best.parameters$)
```

